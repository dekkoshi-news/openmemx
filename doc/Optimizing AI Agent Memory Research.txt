Computational Architectures for Efficient Long-Term Memory in Localized Agentic Systems: A Research Report on Token Reduction and Semantic Abstraction
1. Introduction: The Memory Wall in Localized Agentic AI
The trajectory of Artificial Intelligence is undergoing a seismic shift from ephemeral, stateless request-response paradigms to persistent, stateful interactions driven by autonomous agents. In this emerging landscape, the "agent" is no longer merely a text prediction engine but a longitudinal entity expected to reason, plan, and execute multi-step workflows over extended temporal horizons. This transition has precipitated a critical infrastructural bottleneck: the "Memory Wall." As agents accumulate observations, internal monologues, user interactions, and environmental feedback, their context windows—the working memory of the Transformer architecture—become saturated. While proprietary frontier models have begun to boast context windows exceeding one million tokens, the computational cost, which scales quadratically in standard attention mechanisms, and the associated latency and financial overhead, remain prohibitive for localized, open-source deployments.
This report serves as a critical architectural analysis for the development of an open-source software stack designed to optimize token consumption and memory management for personal AI assistants. Unlike server-side deployments where massive clusters of H100 GPUs can mask inefficiencies through brute force, localized agents running on consumer hardware—such as Apple Silicon MacBooks or NVIDIA RTX-series consumer GPUs—operate under strict thermal, power, and memory bandwidth constraints. Therefore, the solution to the "Memory Wall" for localized agents is not merely to expand the context window, but to fundamentally re-engineer how information is compressed, stored, retrieved, and forgotten.
The analysis synthesizes findings across diverse domains, ranging from information-theoretic memory decay algorithms rooted in Bayesian surprise to hardware-aware inference optimizations like PagedAttention and Neural Processing Unit (NPU) offloading. We posit that a robust solution requires a paradigm shift towards "hierarchical semantic abstraction." Just as biological systems do not retain raw sensory input but rather compressed, semantic representations of events—discarding the noise of the mundane to prioritize the signal of the novel—efficient AI agents must employ active memory management strategies.
The objective of this research is to move beyond the simplistic "vector database plus LLM" pattern that dominates current RAG (Retrieval-Augmented Generation) implementations. We critically evaluate the limitations of current approaches, such as the lack of multi-hop reasoning in vector retrieval and the latency overhead of graph-based methods, to propose a hybrid architecture. This "OpenMemX" architecture integrates surprise-minimization-based forgetting, graph-structured knowledge representation (GraphRAG), and aggressive yet semantically faithful prompt compression (LLMLingua) to enable infinite effective memory within finite compute budgets.
2. Optimizing Context: The Physics of Token Reduction
The first line of defense against context saturation is the efficient management of the prompt itself. Before memory is even retrieved from an external store, the representation of the immediate context must be optimized to ensure that every token passed to the inference engine carries maximum informational value. Research indicates that simply injecting enormous contexts into every prompt leads to high inference latency and quality degradation; if a user is waiting 10 or 15 seconds for a response due to massive context loading, the system is failing in production.1
2.1 Critical Analysis of Prompt Compression Architectures
The field of prompt compression has bifurcated into two primary methodologies: heuristic entropy-based pruning and learned compression. A critical evaluation reveals that for agentic systems, the former is often insufficient, while the latter offers a viable path toward high-fidelity context preservation.
2.1.1 The Limitations of Entropy-Based Pruning
Early approaches to prompt compression, such as Selective Context, operate on the hypothesis that natural language is inherently redundant and that this redundancy can be statistically modeled and removed. These methods typically calculate the self-information (surprisal) of lexical units using a smaller, causal language model (e.g., LLaMA-7B or GPT-2).2 Tokens with lower perplexity—those that are highly predictable given the preceding context—are deemed less informative and are candidates for pruning.
While this approach is computationally inexpensive, requiring only a forward pass of a small model, it suffers from a fundamental architectural flaw: unidirectional context bias. Causal language models, by definition, attend only to the past tokens to predict the next one. Consequently, a token might be statistically redundant based on the immediate past but crucially important for understanding the subsequent text or the overall semantic arc of the prompt. Selective Context fails to capture the "global" semantic importance of a token relative to the specific query or the document's overall structure.3
Empirical benchmarks indicate that entropy-based methods often disrupt the syntactic coherence of the text. This "broken" syntax can confuse the downstream reasoning model, leading to "altered semantic hallucinations" where the compressed prompt inadvertently steers the LLM toward incorrect conclusions because the pruning process removed vital connective tissues like negations or transition words.4 For an agent that relies on precise instructions and logic, this loss of fidelity is unacceptable.
2.1.2 The Learned Compression Paradigm: LLMLingua
A more robust and theoretically sound approach is represented by the LLMLingua series (LLMLingua, LongLLMLingua, and LLMLingua-2). These frameworks reframe prompt compression not as a heuristic filtering task but as a supervised learning problem, optimizing directly for the preservation of downstream utility.
LLMLingua utilizes a coarse-to-fine strategy equipped with a budget controller. This mechanism ensures that semantic integrity is maintained even at high compression ratios (up to 20x) by dynamically allocating the token budget to the most complex parts of the prompt.6 It introduces the concept of "distribution alignment," where the small compression model is fine-tuned to align its perplexity distributions with the target black-box LLM, ensuring that what is surprising to the compressor is also surprising to the reasoner.
LongLLMLingua advances this by addressing the specific challenge of the "Lost in the Middle" phenomenon—a well-documented failure mode where LLMs struggle to retrieve information located in the middle of a long context window. LongLLMLingua employs a question-aware compression metric, employing contrastive perplexity to identify key information specifically pertinent to the user's query.7 Furthermore, it introduces a document reordering strategy, strategically placing the most relevant compressed segments at the beginning or end of the prompt to mitigate the U-shaped performance curve associated with position bias.7
LLMLingua-2 represents the current state-of-the-art for localized agents. It moves away from proxy metrics like perplexity entirely. Instead, it utilizes a data distillation procedure where a frontier model (like GPT-4) generates "gold standard" compressed texts. A Transformer encoder (specifically a BERT-based architecture) is then trained on this distilled dataset to perform token classification (keep vs. drop).3
Strategic Insight: For our open-source agent architecture, LLMLingua-2 is the superior choice. Its use of a bidirectional encoder (BERT) allows it to capture context from both directions (past and future), unlike the unidirectional causal models used in Selective Context.3 This bidirectional visibility ensures that a token is evaluated based on its role in the entire sentence structure. Moreover, because it formulates compression as a parallel classification task rather than a sequential autoregressive generation task, it offers a significant latency advantage (3x-6x speedup).8 Crucially, it guarantees faithfulness to the original text via extractive compression, avoiding the hallucination risks associated with abstractive summarization approaches.8
2.2 Semantic Highlighting and Tiered Inference
Beyond explicit compression, optimizing how models attend to tokens offers further efficiency gains. The concept of Semantic Highlighting involves utilizing a smaller, highly efficient model (e.g., a 0.6B parameter model) to pre-scan the context. This "scout" model tags or "highlights" relevant segments, effectively filtering the input before it is passed to the larger, more expensive reasoning model.9 This creates a tiered inference system where the heavy computational lifting is strictly reserved for high-value tokens, reducing the overall FLOPs required per query.
2.3 Attention Sinks and Streaming Stability
For agents operating over long sessions, maintaining stability in the KV cache is paramount. Research into StreamingLLM has revealed an interesting phenomenon known as "attention sinks." It turns out that initial tokens in a sequence act as anchors, absorbing a disproportionate amount of attention mass even if they lack specific semantic value. Pruning these initial tokens leads to a catastrophic degradation in perplexity and model performance.
However, retaining just a few of these initial tokens (the "sink") allows the model to maintain stability even when the rest of the context functions as a rolling window of the most recent tokens.10 This finding is critical for local agents: it means we do not need to keep the entire history in the active window.
Optimization Recommendation: A localized agent should implement a Hybrid Context Manager. This manager should enforce a "sink" retention policy (locking the system prompt and the first few conversation turns into the KV cache) while applying LLMLingua-2 based compression to the "middle" context. This approach directly neutralizes the "Lost in the Middle" degradation while maximizing the information density of the limited context window available on consumer GPUs.12
3. Cognitive Architectures: Structuring Long-Term Memory
Storing interaction history in a raw text log or a simple vector store is insufficient for the complex reasoning required of an autonomous agent. The research identifies three competing yet complementary memory architectures: Vector RAG, Knowledge Graphs (GraphRAG), and Hierarchical Memory. A critical analysis suggests that no single approach is sufficient; rather, a composite architecture is required.
3.1 The Limitations of Vector RAG
Vector RAG (Retrieval-Augmented Generation) has become the default standard for AI memory. It relies on embedding chunks of text into high-dimensional vectors and retrieving them based on cosine similarity or Euclidean distance. While highly effective for simple fact retrieval (e.g., "What is the capital of France?"), it fails fundamentally at multi-hop reasoning and global sense-making.13
For instance, consider a query like: "How do the supplier delays mentioned last week impact our Q3 production targets?" This question requires connecting disparate facts:
1. Fact A: "Supplier X is delaying shipment of Component Y." (From last week's log).
2. Fact B: "Component Y is critical for Product Z." (From a technical spec).
3. Fact C: "Product Z accounts for 50% of Q3 production targets." (From a strategy doc).
Vector search typically retrieves chunks about "suppliers" or "production targets" in isolation based on keyword similarity. It misses the connective logic (A implies B, B implies C) because the vector representation compresses semantic meaning into a static embedding that may not capture the specific relational dynamic required.14 The "flat" nature of vector stores lacks the structural scaffolding to support transitive reasoning.
3.2 The Promise and Cost of GraphRAG
GraphRAG addresses the reasoning gap by structuring memory as a Knowledge Graph (KG). It extracts entities (nodes) and relationships (edges) from the text, creating a structured topology of information.
* Mechanism: When data is ingested, an LLM extracts triples (Subject, Predicate, Object) to build the graph. Retrieval can then "walk" the graph to find connected concepts, even if they do not share semantic similarity in vector space. For example, it can traverse the path from Supplier X to Component Y to Product Z, enabling the agent to synthesize the answer.15
* Community Summaries: GraphRAG further enhances retrieval by clustering nodes into communities (using algorithms like Leiden) and pre-generating summaries for each cluster. This allows the agent to answer "global" questions ("What are the main themes in the user's project history?") which are computationally intractable for standard RAG, which would require retrieving and reading every document.14
Critical Critique: While GraphRAG offers superior reasoning capabilities, it introduces significant latency and build-time overhead. The process of extracting entities and relationships is computationally expensive, often requiring multiple LLM calls per document. For a real-time personal assistant, the "write" latency of updating the graph after every user message could be prohibitive, creating a lag between interaction and memory consolidation.17 Furthermore, managing the schema and preventing graph explosion requires careful architectural planning.
3.3 Hierarchical Memory Architectures (H-MEM)
To balance the speed of RAG with the depth of GraphRAG, the research points toward a Hierarchical Memory approach (H-MEM). This architecture mirrors human cognition and operating system design, dividing memory into distinct layers based on access frequency and abstraction level.
* Working Memory (RAM): The immediate context window. This is expensive and finite. It contains the current task, the system prompt, and the most recent messages.
* Episodic Memory (Logs/Time-series): A chronological record of interactions. This is best for temporal queries like "What did we discuss yesterday?" or "What was the last code snippet I generated?".
* Semantic Memory (Hard Disk/Knowledge Base): Consolidated facts and knowledge. This is where GraphRAG shines—storing crystallized knowledge (e.g., "User prefers Python," "Project X is due Friday") rather than raw logs.18
Optimization Strategy: The agent should treat memory consolidation as an asynchronous background process.
1. Fast Path: Immediate interactions are logged to a vector store (or simple text log) for instant recall. This ensures the agent remains responsive.
2. Slow Path (Consolidation): Periodically (e.g., during idle time or "sleep cycles"), a background process "wakes up." It analyzes the recent episodes, extracts entities and relationships, updates the Knowledge Graph, and prunes the raw logs. This decoupling ensures the user experience remains snappy while the agent's long-term memory grows smarter and more structured over time.9 This "sleep" cycle is analogous to biological consolidation, where experiences are transferred from the hippocampus to the neocortex.
4. Storage Substrates: The Backend Battle for Local Efficiency
The choice of the underlying database engine deeply impacts the performance, latency, and scalability of a localized agent. The research compares three primary candidates: SQLite, LanceDB, and Git-based approaches.
4.1 LanceDB vs. SQLite for Vector Search
LanceDB emerges from the analysis as a highly compelling option for localized agents because it operates as an embedded vector database. Unlike client-server databases such as Pinecone, Milvus, or Weaviate, which require running a separate service (often via Docker containers) and introduce network latency, LanceDB runs in-process.20
* Performance: LanceDB leverages the Lance columnar format, which is specifically optimized for machine learning workloads and random access. Benchmarks suggest it can be up to 1000x faster than Parquet for random access patterns.22
* Zero-Copy Access: A critical advantage is its support for zero-copy access, meaning data does not need to be copied from storage to memory (RAM) to be read. This significantly reduces the RAM footprint and latency, which is vital for consumer devices where RAM is shared between the OS, the LLM weights, and the application.23
* Comparison with SQLite-vec: While extensions like sqlite-vec bring vector search capabilities to SQLite, they currently lack the specialized on-disk indexing algorithms (like IVF-PQ or DiskANN) required to scale to millions of vectors efficiently while maintaining low latency. SQLite is excellent for metadata and relational data, but LanceDB is purpose-built for high-dimensional vector handling on local disk.21
4.2 The "Git for Memory" (DiffMem) Proposition
A novel and somewhat counter-intuitive concept emerging in the research is using Git as a memory backend, a technique explored in projects like DiffMem.25
* The Concept: The agent stores its memory as a collection of Markdown files. Every conversation or significant memory update results in a git commit.
* Advantages: This approach provides built-in versioning ("time travel"). An agent can answer questions like "How has my opinion on this topic changed over the last year?" by analyzing the git diff between commits. It offers perfect auditability and human-readability—users can open the repo and edit their agent's memory directly.27
* Critical Limitations: Git is not designed for high-frequency, low-latency writes. Operations like "Garbage collection" (git gc) and updating the index become progressively slower as the number of files increases. A repository with 100,000+ small files (a plausible scenario for a long-running agent logging every thought) can experience significant lag in git status and commit operations, which would stall an interactive agent.28
Recommendation: The optimal storage architecture is a Hybrid Storage Pattern.
* Use SQLite for relational metadata (conversation IDs, timestamps, user settings, tool definitions) due to its rock-solid reliability and ACID compliance.31
* Use LanceDB for the actual embedding storage and semantic retrieval, linking records back to SQLite via foreign keys.
* Use Git as the archival/backup layer, not the hot working memory. The agent can operate on LanceDB/SQLite for real-time interaction and "flush" or "snapshot" its state to a Git repository daily or weekly. This provides the versioning benefits without incurring the latency penalties during active conversation.
5. Computational Efficiency: Hardware and Inference Optimization
Optimizing the memory structure is futile if the inference engine cannot handle the computational load. For localized agents, typically running on consumer GPUs (NVIDIA RTX 3090/4090) or Apple Silicon (M1/M2/M3), VRAM (Video RAM) is the scarcest resource.
5.1 KV Cache Quantization and PagedAttention
In long-context scenarios (e.g., 32k or 128k tokens), the Key-Value (KV) Cache—the memory storing the attention keys and values for previously processed tokens—can grow larger than the model weights themselves.32 Managing this cache is the central challenge of long-context inference.
* PagedAttention (vLLM): Traditional attention algorithms allocate contiguous memory blocks for the KV cache. This leads to massive fragmentation and wasted VRAM (internal fragmentation) because the system must reserve space for the maximum possible sequence length, even if it isn't used. PagedAttention, introduced by vLLM, borrows the concept of virtual memory paging from operating systems. It breaks the KV cache into non-contiguous blocks (pages) managed by a page table. This allows for near-zero memory waste and supports larger batch sizes and significantly longer contexts within the same memory footprint.33
* Quantization (FP8/INT4): Storing the KV cache in FP16 (16-bit floating point) is increasingly viewed as wasteful. Research demonstrates that quantizing the KV cache to FP8 (8-bit floating point) reduces memory usage by 50% with negligible accuracy loss.34 Emerging research into INT4 KV cache suggests further reductions are possible, potentially doubling the context length capacity again. However, this requires specialized kernels (like FireQ) to handle outlier values in attention matrices without degrading model performance. Without outlier smoothing, INT4 quantization can lead to significant perplexity degradation.36
5.2 Apple Silicon Idiosyncrasies: The ANE Trap
For developers targeting the Apple ecosystem, the Unified Memory Architecture (UMA) allows the CPU and GPU to share a massive pool of RAM (up to 128GB+ on M-series Max/Ultra chips). This is a significant advantage over NVIDIA consumer cards, which are capped at 24GB VRAM. However, the Apple Neural Engine (ANE) presents a trap.
The ANE is optimized for static shapes and specific data formats (channels-first), making it hostile to the dynamic Transformers and variable-length sequence operations common in LLMs.38 Developers often find that ANE utilization is capped at <20% for LLM workloads, with the system falling back to the GPU or CPU for unsupported operations. Furthermore, the overhead of moving data between the CPU/GPU and the ANE can negate the compute benefits.
Optimization Insight: For Apple Silicon, optimization efforts should focus on Metal Performance Shaders (MPS) and CoreML quantization (palettization) for the GPU, rather than over-optimizing for the ANE. Until Apple provides better support for dynamic shapes in the ANE, the GPU offers the best balance of bandwidth and compute flexibility for LLMs.39 Tools like MLX are specifically designed to leverage the UMA and GPU on Apple Silicon, offering a more efficient path than trying to force PyTorch models onto the ANE.41
5.3 FlashAttention-3
FlashAttention-3 represents the cutting edge of attention optimization. It exploits the asynchrony of modern GPUs (specifically NVIDIA Hopper H100s) to overlap computation with memory transfer (using mechanisms like TMA and warp-specialization). It achieves 1.5-2.0x speedups over FlashAttention-2.42 While consumer GPUs (Ampere/Ada Lovelace) may not support all features like TMA, the underlying principle of tile-based computation remains critical. It reduces the quadratic complexity of attention to linear memory I/O, which is essential for local agents running 128k+ context windows.43 For local agents, ensuring the inference backend (e.g., llama.cpp or vLLM) supports FlashAttention-2 or 3 is non-negotiable for performance.
6. Semantic Abstraction: Algorithms for Forgetting
An "infinite" memory system that never forgets is inefficient and arguably misaligned with the definition of intelligence. Biological memory relies heavily on forgetting mechanisms to maintain relevance and efficiency. The research suggests leveraging Information Theory to engineer "memory decay" algorithms that mimic this biological pruning.
6.1 Surprise Minimization and Bayesian Surprise
The core metric for determining what to keep and what to forget is Bayesian Surprise. This metric measures the difference (specifically, the Kullback-Leibler divergence) between a model's prior belief and its posterior belief after observing new data.44
Concept: Data that drastically changes the agent's internal model or expectations is "surprising" and therefore possesses high informational value. It should be retained. Data that conforms to expectations (low surprise) is redundant and can be compressed or discarded without significant loss of information.
Mathematical Formulation:
The surprise    elicited by data    given a model    is defined as:
$$ S(D, M) = KL(P(M|D) |
| P(M)) $$
Where:
*    is the prior probability distribution of the model parameters (the agent's current belief state).
*    is the posterior distribution after observing the new data   .44
Implementation: An agent can calculate the "surprise score" of an incoming interaction.
   * High Surprise: The event significantly alters the agent's understanding. Store this in Episodic Memory with high priority and trigger a GraphRAG update to revise the semantic world model.
   * Low Surprise: The event is predictable (e.g., a standard greeting or a repeated command). Prune this immediately or summarize it into a generic pattern to save tokens.
By implementing a Surprise-based Memory Pruning algorithm, an agent can dynamically adjust its memory retention based on the informational value of events rather than just their recency (Least Recently Used - LRU) or frequency. This prevents the "memory bank" from filling up with trivial data while preserving critical but infrequent updates (e.g., "I moved to a new apartment").47
7. Evaluation: Beyond "Needle in a Haystack"
To validate these architectural choices, we must move beyond standard benchmarks like "Needle in a Haystack" (NIAH), which only test simple retrieval. An agent might retrieve a fact but fail to understand its implications over time.
   * RULER Benchmark: This suite expands on NIAH by introducing variable tracking, multi-hop tracing, and aggregation tasks. It reveals that many models claiming 32k+ context windows degrade significantly in reasoning quality well before reaching that limit. It tests if a model can track a variable's state changes across a long context, which is a proxy for tracking the state of a conversation or project.49
   * InfiniteBench: This benchmark focuses on contexts exceeding 100k tokens, testing whether models can maintain coherence and reasoning over "novel-length" inputs. It is crucial for agents expected to ingest entire codebases or long documents.49
   * LOCOMO: Specifically designed for long-term conversational memory, LOCOMO evaluates consistency over multi-session interactions. It tests whether an agent can recall details from several sessions ago and use them to inform current responses. This is the closest proxy to the "personal assistant" use case.52
Implication: Any open-source memory software must be rigorously validated against RULER (for effective context length reasoning) and LOCOMO (for longitudinal consistency). A system might pass NIAH (retrieval) but fail RULER (reasoning), leading to an agent that "remembers" facts but cannot use them to solve problems.54
8. Proposed Architecture: The "OpenMemX" Stack
Based on this synthesis, we propose the "OpenMemX" stack—a concrete architectural blueprint for an open-source, token-optimized agentic memory system.
8.1 The Storage Layer (Hybrid)
   * Hot Memory: LanceDB (embedded, zero-copy) is used for vector storage and semantic retrieval of the most recent and relevant chunks.
   * Structured Memory: SQLite handles metadata, user profiles, conversation logs, and the relational mapping of graph nodes.
   * Archival Memory: Git (managed via a library like libgit2, not the CLI) is used for daily snapshots and "time-travel" versioning, providing a backup and a history of the agent's knowledge evolution.
8.2 The Processing Layer (Hierarchical)
   1. Ingress: User input is processed via LLMLingua-2. This step compresses the prompt, removing 20-40% of tokens without semantic loss, effectively expanding the virtual context window.
   2. Working Memory: A "sliding window" combined with an "attention sink" buffer keeps the system prompt and the immediate conversation active in the KV cache.
   3. Recall:
   * Level 1: Dense Vector Search (via LanceDB) retrieves the top-k relevant chunks.
   * Level 2: If the query implies complex reasoning (detected via a lightweight classifier), the system triggers a GraphRAG traversal to find multi-hop connections in the semantic memory.
   4. Consolidation (The "Sleep" Cycle): An asynchronous process runs in the background. It calculates Bayesian Surprise on recent interactions. High-surprise events are extracted into the Knowledge Graph; low-surprise events are summarized or discarded.
8.3 The Inference Layer (Local Optimization)
   * The system should utilize vLLM with PagedAttention to maximize memory utilization and reduce fragmentation.
   * FP8 KV Cache quantization should be enabled by default to double the effective context capacity of consumer GPUs.
   * On Apple Silicon, the stack should leverage MLX or optimized CoreML pipelines that utilize the GPU bandwidth effectively, accepting the current limitations of the ANE for dynamic workloads.
9. Conclusion
The future of personal AI agents lies not in brute-forcing larger context windows, but in smarter, more biologically inspired context management. "Just add more context" is a strategy of diminishing returns that hits hard hardware walls.55 By shifting from passive storage to active, surprise-driven memory curation, and by adopting hierarchical architectures that blend the speed of vectors with the reasoning of graphs, we can build agents that are not only token-efficient but deeply intelligent.
The proposed "OpenMemX" stack offers a viable path forward for the open-source community. It democratizes the capability for long-horizon, reasoning-capable agents, allowing them to run locally on the devices we own, preserving privacy while delivering a persistent and evolving user experience.
________________
Table 1: Comparative Analysis of Memory Backend Technologies
Feature
	SQLite
	LanceDB
	Git / Markdown
	Primary Data Type
	Relational / Structured
	Vectors / High-Dimensional
	Unstructured Text / Logs
	Search Capability
	Exact Match / SQL
	Semantic / Vector Similarity
	Text Search / Diff Analysis
	Latency (Read)
	Microseconds (In-Memory)
	Milliseconds (Zero-Copy)
	Slow (File I/O dependent)
	Latency (Write)
	Fast (WAL enabled)
	Fast (Append-only)
	Slow (Commit overhead)
	Scalability
	High (Billions of rows)
	Very High (Billions of vectors)
	Low (Filesystem limits)
	Versioning
	Complex (requires schema)
	Natively supported (Time travel)
	Native (Core feature)
	Agent Suitability
	Metadata & Relations
	Semantic Retrieval
	Archival & Audit
	Table 2: Comparison of Prompt Compression Methods
Method
	Mechanism
	Context Scope
	Faithfulness
	Latency
	Best Use Case
	Selective Context
	Entropy pruning (PPL)
	Unidirectional (Left-to-Right)
	Low (Breaks syntax)
	Low
	Simple tasks
	LLMLingua
	Coarse-to-fine budget control
	Unidirectional
	Medium
	Medium
	General Compression
	LongLLMLingua
	Question-aware contrastive PPL
	Unidirectional
	High
	High
	RAG / Long Context
	LLMLingua-2
	Token Classification (BERT)
	Bidirectional
	Very High (Extractive)
	Very Low
	Production Agents
	Works cited
   1. A Practical Deep Dive Into Memory Optimization for Agentic Systems (Part A), accessed February 2, 2026, https://www.dailydoseofds.com/ai-agents-crash-course-part-15-with-implementation/
   2. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models, accessed February 2, 2026, https://arxiv.org/html/2310.05736v2
   3. Learn Compression Target via Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression - LLMLingua-2, accessed February 2, 2026, https://llmlingua.com/llmlingua2.html
   4. Prompt Compression in Large Language Models (LLMs): Making Every Token Count | by Sahin Ahmed, Data Scientist | Medium, accessed February 2, 2026, https://medium.com/@sahin.samia/prompt-compression-in-large-language-models-llms-making-every-token-count-078a2d1c7e03
   5. An Empirical Study on Prompt Compression for Large Language Models - arXiv, accessed February 2, 2026, https://arxiv.org/html/2505.00019v1
   6. Compressing Prompts for Accelerated Inference of Large Language Models - LLMLingua, accessed February 2, 2026, https://llmlingua.com/llmlingua.html
   7. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression, accessed February 2, 2026, https://llmlingua.com/longllmlingua.html
   8. LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression - ACL Anthology, accessed February 2, 2026, https://aclanthology.org/2024.findings-acl.57.pdf
   9. SimpleMem: Efficient Lifelong Memory for LLM Agents - arXiv, accessed February 2, 2026, https://arxiv.org/html/2601.02553v3
   10. Efficient Streaming Language Models with Attention Sinks - arXiv, accessed February 2, 2026, https://arxiv.org/html/2309.17453v4
   11. Attention Sinks in LLMs for endless fluency - Hugging Face, accessed February 2, 2026, https://huggingface.co/blog/tomaarsen/attention-sinks
   12. KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs | OpenReview, accessed February 2, 2026, https://openreview.net/forum?id=gIqb6zWZoO
   13. RAG vs GraphRAG: Shared Goal & Key Differences - Memgraph, accessed February 2, 2026, https://memgraph.com/blog/rag-vs-graphrag
   14. What is GraphRAG? - IBM, accessed February 2, 2026, https://www.ibm.com/think/topics/graphrag
   15. RAG: Graph Retrieval vs Graph Reasoning | by Chia Jeng Yang - Medium, accessed February 2, 2026, https://medium.com/enterprise-rag/rag-graph-retrieval-vs-graph-reasoning-814137f61c5a
   16. Procedural Memory Graph - GraphRAG, accessed February 2, 2026, https://graphrag.com/reference/knowledge-graph/memory-graph-procedural/
   17. How to Design Efficient Memory Architectures for Agentic AI Systems - Towards AI, accessed February 2, 2026, https://pub.towardsai.net/how-to-design-efficient-memory-architectures-for-agentic-ai-systems-81ed456bb74f
   18. What is AI Agent Memory? Architectures, vector stores, and GraphRAG - Mem0, accessed February 2, 2026, https://mem0.ai/blog/what-is-ai-agent-memory
   19. Semantic Memory Graph - GraphRAG, accessed February 2, 2026, https://graphrag.com/reference/knowledge-graph/memory-graph-semantic/
   20. Bottomless vector database storage with Tigris and LanceDB, accessed February 2, 2026, https://www.tigrisdata.com/blog/lancedb-101/
   21. Compare LanceDB vs. SQLite in 2026 - Slashdot, accessed February 2, 2026, https://slashdot.org/software/comparison/LanceDB-vs-SQLite/
   22. LanceDB: Frequently Asked Questions, accessed February 2, 2026, https://docs.lancedb.com/faq/faq-oss
   23. The Future of AI-Native Development is Local: Inside Continue's LanceDB-Powered Evolution, accessed February 2, 2026, https://lancedb.com/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/
   24. Created SurgeDB A fully local, filter-capable vector search engine. : r/SideProject - Reddit, accessed February 2, 2026, https://www.reddit.com/r/SideProject/comments/1qkod7z/created_surgedb_a_fully_local_filtercapable/
   25. Show HN: I replaced vector databases with Git for AI memory (PoC) | Hacker News, accessed February 2, 2026, https://news.ycombinator.com/item?id=44969622
   26. AI Field Notes: Commit Early and Often, AI is Great with Git | by Ryan Detzel | Medium, accessed February 2, 2026, https://medium.com/@ryandetzel/ai-field-notes-commit-early-and-often-ai-is-great-with-git-d8379176cb57
   27. DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC, accessed February 2, 2026, https://www.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/
   28. Put 10000 small text files to code base, will it make git slow? - Stack ..., accessed February 2, 2026, https://stackoverflow.com/questions/20974714/put-10000-small-text-files-to-code-base-will-it-make-git-slow
   29. Repository limits - GitHub Docs, accessed February 2, 2026, https://docs.github.com/en/repositories/creating-and-managing-repositories/repository-limits
   30. Does too many commits affect performance? : r/git - Reddit, accessed February 2, 2026, https://www.reddit.com/r/git/comments/vpbmbe/does_too_many_commits_affect_performance/
   31. Why Use SQL Databases for AI Agent Memory - GibsonAI, accessed February 2, 2026, https://gibsonai.com/blog/why-use-sql-databases-for-ai-agent-memory
   32. INT4 Decoding GQA CUDA Optimizations for LLM Inference - PyTorch, accessed February 2, 2026, https://pytorch.org/blog/int4-decoding/
   33. PagedAttention: Efficient Memory Management for LLMs - Emergent Mind, accessed February 2, 2026, https://www.emergentmind.com/topics/pagedattention
   34. Optimizing Inference for Long Context and Large Batch Sizes with NVFP4 KV Cache, accessed February 2, 2026, https://developer.nvidia.com/blog/optimizing-inference-for-long-context-and-large-batch-sizes-with-nvfp4-kv-cache/
   35. Quantized KV Cache - vLLM, accessed February 2, 2026, https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/
   36. FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration, accessed February 2, 2026, https://arxiv.org/html/2505.20839v1
   37. FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration, accessed February 2, 2026, https://arxiv.org/html/2505.20839v2
   38. Leveraging Attention to Effectively Compress Prompts for Long-Context LLMs, accessed February 2, 2026, https://ojs.aaai.org/index.php/AAAI/article/download/34800/36955
   39. [D] Anyone successfully running LLMs fully on Apple Neural Engine (ANE)? - Reddit, accessed February 2, 2026, https://www.reddit.com/r/MachineLearning/comments/1n1pcj7/d_anyone_successfully_running_llms_fully_on_apple/
   40. Deploying Transformers on the Apple Neural Engine - Apple Machine Learning Research, accessed February 2, 2026, https://machinelearning.apple.com/research/neural-engine-transformers
   41. Exploring LLMs with MLX and the Neural Accelerators in the M5 GPU, accessed February 2, 2026, https://machinelearning.apple.com/research/exploring-llms-mlx-m5
   42. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision - NIPS, accessed February 2, 2026, https://proceedings.neurips.cc/paper_files/paper/2024/file/7ede97c3e082c6df10a8d6103a2eebd2-Paper-Conference.pdf
   43. Dao-AILab/flash-attention: Fast and memory-efficient exact attention - GitHub, accessed February 2, 2026, https://github.com/Dao-AILab/flash-attention
   44. Exploration, novelty, surprise, and free energy minimization - Frontiers, accessed February 2, 2026, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.00710/full
   45. Surprise minimization as a learning strategy in neural networks - PMC - PubMed Central, accessed February 2, 2026, https://pmc.ncbi.nlm.nih.gov/articles/PMC4697468/
   46. Of Bits and Wows: A Bayesian Theory of Surprise with Applications to Attention - PMC, accessed February 2, 2026, https://pmc.ncbi.nlm.nih.gov/articles/PMC2860069/
   47. EM-LLM: Human-inspired Episodic Memory for Infinite Context LLMs - GitHub, accessed February 2, 2026, https://github.com/em-llm/EM-LLM-model
   48. Human-like Episodic Memory for Infinite Context LLMs - arXiv, accessed February 2, 2026, https://arxiv.org/html/2407.09450v1
   49. [2404.06654] RULER: What's the Real Context Size of Your Long-Context Language Models? - arXiv, accessed February 2, 2026, https://arxiv.org/abs/2404.06654
   50. How to evaluate the "true" context length of your LLM using RULER | ruler_eval - Wandb, accessed February 2, 2026, https://wandb.ai/byyoung3/ruler_eval/reports/How-to-evaluate-the-true-context-length-of-your-LLM-using-RULER---VmlldzoxNDE0OTA0OQ
   51. [2402.13718] $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens - arXiv, accessed February 2, 2026, https://arxiv.org/abs/2402.13718
   52. Evaluating Very Long-Term Conversational Memory of LLM Agents, accessed February 2, 2026, https://snap-research.github.io/locomo/
   53. LOCOMO Benchmark: Long-Horizon Memory in LLMs - Emergent Mind, accessed February 2, 2026, https://www.emergentmind.com/topics/locomo-benchmark-d06cff1a-d4a5-4df8-ab85-fdca157d190b
   54. RULER: Benchmark to evaluate long-context modeling capabilities of language models | by SACHIN KUMAR | Medium, accessed February 2, 2026, https://medium.com/@techsachin/ruler-benchmark-to-evaluate-long-context-modeling-capabilities-of-language-models-7eb13a269e36
   55. A 2026 Memory Stack for Enterprise Agents - Alok Mishra, accessed February 2, 2026, https://alok-mishra.com/2026/01/07/a-2026-memory-stack-for-enterprise-agents/